{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfkKz+8HC+aIcs6WVo8iaq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuugiouduele/AImodel/blob/main/Parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "Vdbed_Im_DgG",
        "outputId": "1d722ed0-4628-43e6-9444-76e530388a70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Upload a PDF file (or cancel to use built-in dummy)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-031fabef-3946-4ed7-bdd4-1d919ba69280\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-031fabef-3946-4ed7-bdd4-1d919ba69280\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 2012_JSLIS_Spring_webir122.pdf to 2012_JSLIS_Spring_webir122.pdf\n",
            "Using PDF: 2012_JSLIS_Spring_webir122.pdf\n",
            "Extracted 4 pages\n",
            "Saved global_frequency.csv\n",
            "Saved term_dictionary.json and term_summary.csv\n",
            "Saved symbol_list.json\n",
            "Saved heatmap as term_heatmap.png using matplotlib\n",
            "Saved term_page_heatmap.csv\n",
            "Saved bar chart as top15_frequency.png using matplotlib\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4068311851.py:77: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
            "  ax.set_xticklabels(labels, rotation=45, ha='right')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved wordcloud image to term_frequency_wordcloud.png\n",
            "Saved candidate_method_paragraphs.txt (for manual review)\n",
            "Saved parsing_report.md\n",
            "All done. Inspect CSV/JSON outputs and review candidate method paragraphs before using LLM to generate definitions.\n"
          ]
        }
      ],
      "source": [
        "# Google Colab-ready: Paper parsing & precision-reading assistant pipeline (DEBUGGED)\n",
        "# -----------------------------------------------------------------\n",
        "# Goal:\n",
        "# - Load a PDF (user-uploaded) or use a built-in dummy PDF\n",
        "# - Extract text, equations, figure/table captions, and page/section boundaries\n",
        "# - Produce: frequent-term ranking, term-definition dictionary (with contexts),\n",
        "#   symbol list (variables, math tokens), and a heatmap of term occurrences by section/page\n",
        "# - Provide helper functions to call the OpenAI (ChatGPT) API to fetch definitions/explanations\n",
        "# - This notebook is intended as a runnable Colab script; it is robust to missing plotting\n",
        "#   libraries (matplotlib) and will fall back to plotly or CSV outputs when necessary.\n",
        "\n",
        "# USAGE in Colab:\n",
        "# 1. Upload your PDF using the file upload widget or mount Google Drive\n",
        "# 2. Run each cell in order. The pipeline will create CSV/PNG/HTML outputs and a small report.\n",
        "# 3. Replace the OpenAI API key placeholder or set it as an environment variable if you want\n",
        "#    automated LLM-based definitions (recommended: review them manually).\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Install dependencies (best-effort). If you run in a locked environment, you can skip this cell\n",
        "# and ensure the listed packages exist in your environment.\n",
        "!pip install --quiet PyMuPDF pdfplumber spacy scikit-learn wordcloud openai python-dateutil matplotlib plotly kaleido reportlab\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Imports (deferred plotting imports are handled in helper functions)\n",
        "import os\n",
        "import re\n",
        "import io\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import pdfplumber\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# WordCloud and spaCy\n",
        "from wordcloud import WordCloud\n",
        "import spacy\n",
        "\n",
        "# Optional OpenAI client\n",
        "try:\n",
        "    import openai\n",
        "except Exception:\n",
        "    openai = None\n",
        "\n",
        "# Load spaCy model (downloaded above)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Utilities: robust plotting helpers that gracefully handle missing matplotlib\n",
        "\n",
        "def _detect_plotting_backend():\n",
        "    \"\"\"Return plotting backend: 'matplotlib', 'plotly', or None\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as _plt  # type: ignore\n",
        "        return 'matplotlib'\n",
        "    except Exception:\n",
        "        try:\n",
        "            import plotly.express as _px  # type: ignore\n",
        "            import plotly.graph_objects as _go  # type: ignore\n",
        "            return 'plotly'\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "PLOTTING_BACKEND = _detect_plotting_backend()\n",
        "\n",
        "\n",
        "def save_bar_chart(labels: List[str], values: List[int], filepath: str):\n",
        "    \"\"\"Save a bar chart to filepath. If matplotlib not available, try plotly; otherwise save CSV.\"\"\"\n",
        "    backend = PLOTTING_BACKEND\n",
        "    if backend == 'matplotlib':\n",
        "        import matplotlib.pyplot as plt\n",
        "        fig, ax = plt.subplots(figsize=(max(6, len(labels)*0.4), 4))\n",
        "        ax.bar(labels, values)\n",
        "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
        "        ax.set_ylabel('Frequency')\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(filepath)\n",
        "        plt.close(fig)\n",
        "        print(f'Saved bar chart as {filepath} using matplotlib')\n",
        "    elif backend == 'plotly':\n",
        "        import plotly.express as px\n",
        "        fig = px.bar(x=labels, y=values, labels={'x':'Word','y':'Frequency'})\n",
        "        # Prefer image export, fall back to HTML\n",
        "        try:\n",
        "            fig.write_image(filepath)\n",
        "            print(f'Saved bar chart as {filepath} using plotly (image)')\n",
        "        except Exception:\n",
        "            html_path = filepath.replace('.png', '.html')\n",
        "            fig.write_html(html_path)\n",
        "            print(f'Plotly image export not available. Saved interactive HTML at {html_path}')\n",
        "    else:\n",
        "        # No plotting library available\n",
        "        df = pd.DataFrame({'label': labels, 'value': values})\n",
        "        csv_path = filepath.replace('.png', '.csv')\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f'No plotting library available. Saved CSV at {csv_path}')\n",
        "\n",
        "\n",
        "def save_heatmap(matrix: np.ndarray, row_labels: List[str], col_labels: List[str], filepath: str):\n",
        "    \"\"\"Save heatmap image. If not possible, save CSV matrix.\"\"\"\n",
        "    backend = PLOTTING_BACKEND\n",
        "    if backend == 'matplotlib':\n",
        "        import matplotlib.pyplot as plt\n",
        "        fig, ax = plt.subplots(figsize=(max(6, len(col_labels)*0.6), max(4, len(row_labels)*0.3)))\n",
        "        cax = ax.imshow(matrix, aspect='auto', cmap='YlGnBu')\n",
        "        fig.colorbar(cax, ax=ax, label='count')\n",
        "        ax.set_yticks(range(len(row_labels)))\n",
        "        ax.set_yticklabels(row_labels)\n",
        "        ax.set_xticks(range(len(col_labels)))\n",
        "        ax.set_xticklabels(col_labels, rotation=90)\n",
        "        ax.set_xlabel('Page')\n",
        "        fig.tight_layout()\n",
        "        fig.savefig(filepath)\n",
        "        plt.close(fig)\n",
        "        print(f'Saved heatmap as {filepath} using matplotlib')\n",
        "    elif backend == 'plotly':\n",
        "        import plotly.express as px\n",
        "        fig = px.imshow(matrix, labels=dict(x='Page', y='Term', color='count'), x=col_labels, y=row_labels)\n",
        "        try:\n",
        "            fig.write_image(filepath)\n",
        "            print(f'Saved heatmap as {filepath} using plotly (image)')\n",
        "        except Exception:\n",
        "            html_path = filepath.replace('.png', '.html')\n",
        "            fig.write_html(html_path)\n",
        "            print(f'Plotly image export not available. Saved interactive HTML at {html_path}')\n",
        "    else:\n",
        "        # fallback to CSV\n",
        "        df = pd.DataFrame(matrix, index=row_labels, columns=col_labels)\n",
        "        csv_path = filepath.replace('.png', '.csv')\n",
        "        df.to_csv(csv_path)\n",
        "        print(f'No plotting library available. Saved matrix CSV at {csv_path}')\n",
        "\n",
        "\n",
        "def save_wordcloud_from_frequencies(freq_series: pd.Series, filepath: str):\n",
        "    \"\"\"Generate and save a wordcloud from frequency series (word->freq). Uses WordCloud.to_file (Pillow required).\n",
        "    If that fails, save frequencies as CSV instead.\"\"\"\n",
        "    try:\n",
        "        wc = WordCloud(width=1200, height=600)\n",
        "        wc.generate_from_frequencies(freq_series.to_dict())\n",
        "        wc.to_file(filepath)\n",
        "        print(f'Saved wordcloud image to {filepath}')\n",
        "    except Exception as e:\n",
        "        csv_path = filepath.replace('.png', '.csv')\n",
        "        freq_series.to_frame('frequency').to_csv(csv_path)\n",
        "        print(f'Could not save wordcloud image ({e}). Saved frequencies CSV at {csv_path}')\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Helper: create a small dummy PDF if none is provided (safe for commercial use)\n",
        "\n",
        "def create_dummy_pdf(path: str):\n",
        "    try:\n",
        "        from reportlab.lib.pagesizes import A4\n",
        "        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle\n",
        "        from reportlab.lib.styles import getSampleStyleSheet\n",
        "        from reportlab.lib import colors\n",
        "    except Exception:\n",
        "        raise RuntimeError('reportlab not available to create dummy PDF. Please upload a PDF.')\n",
        "\n",
        "    styles = getSampleStyleSheet()\n",
        "    doc = SimpleDocTemplate(path, pagesize=A4)\n",
        "    story = []\n",
        "    story.append(Paragraph('<b>Bayesian Spatio-Temporal Forecasting for Synthetic Weather Patterns</b>', styles['Title']))\n",
        "    story.append(Spacer(1, 12))\n",
        "    story.append(Paragraph('<b>Abstract</b>', styles['Heading2']))\n",
        "    story.append(Paragraph('This is a synthetic dummy paper generated for testing the parsing pipeline. It contains repeated keywords and a few math-like tokens.', styles['BodyText']))\n",
        "    story.append(Spacer(1, 12))\n",
        "    story.append(Paragraph('<b>1. Introduction</b>', styles['Heading2']))\n",
        "    story.append(Paragraph('Intro with keywords: Bayesian, temporal, latent, filter, weather, synthetic, Markov, Gaussian, model, variable.', styles['BodyText']))\n",
        "    story.append(Spacer(1, 12))\n",
        "    story.append(Paragraph('<b>2. Method</b>', styles['Heading2']))\n",
        "    story.append(Paragraph('We define X_t, z_t and assume P(Y_t | X_t) = N(X_t, sigma^2) and P(X_t | X_{t-1}) = GP(X_{t-1}, k(.,.))', styles['BodyText']))\n",
        "    story.append(Spacer(1, 12))\n",
        "    story.append(Paragraph('<b>3. Results</b>', styles['Heading2']))\n",
        "    story.append(Paragraph('Results mention RMSE, AUC, ROC and comparison to Kalman Filter and Markov Chain.', styles['BodyText']))\n",
        "    story.append(Spacer(1, 12))\n",
        "    data = [['Model', 'RMSE', 'AUC'], ['Markov Chain', '3.45', '0.81'], ['Kalman Filter', '3.12', '0.85'], ['Proposed Bayesian Filter', '2.94', '0.92']]\n",
        "    table = Table(data)\n",
        "    table.setStyle(TableStyle([('BACKGROUND', (0, 0), (-1, 0), colors.lightgrey), ('GRID', (0, 0), (-1, -1), 1, colors.black)]))\n",
        "    story.append(table)\n",
        "    story.append(Spacer(1, 12))\n",
        "    story.append(Paragraph('<b>4. Discussion</b>', styles['Heading2']))\n",
        "    story.append(Paragraph('Discussion with keywords: hierarchical, forecasting, atmospheric, modeling, latent.', styles['BodyText']))\n",
        "    doc.build(story)\n",
        "    print(f'Created dummy PDF at {path}')\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# PDF input: upload in Colab or use local path\n",
        "\n",
        "try:\n",
        "    # Try Colab upload widget\n",
        "    from google.colab import files\n",
        "    print('Upload a PDF file (or cancel to use built-in dummy)')\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        pdf_path = list(uploaded.keys())[0]\n",
        "    else:\n",
        "        pdf_path = '/content/dummy_paper.pdf'\n",
        "        if not os.path.exists(pdf_path):\n",
        "            create_dummy_pdf(pdf_path)\n",
        "except Exception:\n",
        "    # Not running in Colab; try to use local path or create dummy\n",
        "    pdf_path = 'dummy_paper.pdf'\n",
        "    if not os.path.exists(pdf_path):\n",
        "        create_dummy_pdf(pdf_path)\n",
        "\n",
        "print(f'Using PDF: {pdf_path}')\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Function: extract raw text on a per-page basis using pdfplumber (better layout) and fallback to fitz\n",
        "\n",
        "def extract_text_per_page(path: str) -> List[str]:\n",
        "    pages = []\n",
        "    try:\n",
        "        with pdfplumber.open(path) as pdf:\n",
        "            for p in pdf.pages:\n",
        "                text = p.extract_text() or \"\"\n",
        "                pages.append(text)\n",
        "    except Exception as e:\n",
        "        # fallback to PyMuPDF\n",
        "        doc = fitz.open(path)\n",
        "        for p in doc:\n",
        "            pages.append(p.get_text())\n",
        "    return pages\n",
        "\n",
        "pages = extract_text_per_page(pdf_path)\n",
        "print(f'Extracted {len(pages)} pages')\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Section splitter (heuristic): split by headings, long-line patterns, or page boundaries\n",
        "\n",
        "def split_into_sections(pages: List[str]) -> List[Dict]:\n",
        "    sections = []\n",
        "    for i, text in enumerate(pages):\n",
        "        if not text:\n",
        "            sections.append({'page': i+1, 'section_title': None, 'text': ''})\n",
        "            continue\n",
        "        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
        "        title = None\n",
        "        if lines:\n",
        "            if re.match(r'^(abstract|introduction|method|results|discussion|references)', lines[0].lower()):\n",
        "                title = lines[0]\n",
        "        sections.append({'page': i+1, 'section_title': title, 'text': text})\n",
        "    return sections\n",
        "\n",
        "sections = split_into_sections(pages)\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Tokenization & cleanup utilities\n",
        "stopwords = set([\n",
        "    'the','and','for','of','to','a','in','on','is','we','our','was','by','this','with','at','an','as','be','are','that','it','from','both','which','these','have','has','or'\n",
        "])\n",
        "\n",
        "word_re = re.compile(r\"[A-Za-z][A-Za-z0-9_\\-]{2,}\")\n",
        "\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    text = re.sub(r\"[^A-Za-z0-9_\\-]\", \" \", text)\n",
        "    tokens = [t.lower() for t in word_re.findall(text)]\n",
        "    tokens = [t for t in tokens if t not in stopwords]\n",
        "    return tokens\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# 1) Frequent-term ranking (global / per-section)\n",
        "\n",
        "def compute_frequencies(sections: List[Dict], topk: int=50) -> Tuple[pd.DataFrame, Dict[int, pd.DataFrame]]:\n",
        "    global_counter = Counter()\n",
        "    per_page_counters = {}\n",
        "    for s in sections:\n",
        "        tokens = tokenize(s['text'] or '')\n",
        "        c = Counter(tokens)\n",
        "        per_page_counters[s['page']] = pd.DataFrame(c.items(), columns=['word','freq']).sort_values('freq', ascending=False)\n",
        "        global_counter.update(c)\n",
        "    global_df = pd.DataFrame(global_counter.items(), columns=['word','frequency']).sort_values('frequency', ascending=False).reset_index(drop=True)\n",
        "    return global_df.head(topk), per_page_counters\n",
        "\n",
        "global_freq_df, per_page = compute_frequencies(sections, topk=200)\n",
        "\n",
        "# Save CSV for inspection\n",
        "global_freq_df.to_csv('global_frequency.csv', index=False)\n",
        "print('Saved global_frequency.csv')\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# 2) Term dictionary (definitions pulled from local context + optional external API)\n",
        "\n",
        "def extract_contexts_for_term(term: str, sections: List[Dict], window_chars: int=200) -> Dict:\n",
        "    contexts = []\n",
        "    pages_set = set()\n",
        "    pattern = re.compile(r\"\\\\b\" + re.escape(term) + r\"\\\\b\", flags=re.IGNORECASE)\n",
        "    # Use raw word boundaries properly (note: compile with python's \\b)\n",
        "    # But if the term contains punctuation, re.escape ensures correct matching\n",
        "    for s in sections:\n",
        "        text = s['text'] or ''\n",
        "        for m in pattern.finditer(text):\n",
        "            start = max(0, m.start()-window_chars)\n",
        "            end = min(len(text), m.end()+window_chars)\n",
        "            snippet = text[start:end].replace('\\n', ' ')\n",
        "            contexts.append(snippet)\n",
        "            pages_set.add(s['page'])\n",
        "    freq_val = int(global_freq_df.loc[global_freq_df['word']==term,'frequency'].iloc[0]) if term in set(global_freq_df['word']) else 0\n",
        "    return {'term': term, 'freq': freq_val, 'pages': sorted(list(pages_set)), 'contexts': contexts}\n",
        "\n",
        "# Build preliminary term dictionary for top K\n",
        "TOP_K = 40\n",
        "term_dict = []\n",
        "for term in global_freq_df['word'].head(TOP_K).tolist():\n",
        "    term_dict.append(extract_contexts_for_term(term, sections))\n",
        "\n",
        "# Save as JSON and CSV summary\n",
        "with open('term_dictionary.json', 'w') as f:\n",
        "    json.dump(term_dict, f, indent=2)\n",
        "\n",
        "term_summary = pd.DataFrame([{'term': t['term'], 'freq': t['freq'], 'pages': t['pages'], 'num_contexts': len(t['contexts'])} for t in term_dict])\n",
        "term_summary.to_csv('term_summary.csv', index=False)\n",
        "print('Saved term_dictionary.json and term_summary.csv')\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# 3) Symbol / math token extraction\n",
        "\n",
        "def extract_symbols_from_text(text: str) -> List[str]:\n",
        "    syms = set()\n",
        "    # LaTeX inline math\n",
        "    for m in re.finditer(r\"\\$([^$]{1,200})\\$\", text):\n",
        "        syms.update(re.findall(r\"[A-Za-z\\\\]+(?:_[a-zA-Z0-9]+)?(?:\\^\\{?[0-9a-zA-Z]+\\}?)?\", m.group(1)))\n",
        "    # subscripts/superscripts like X_t, z_t, sigma^2\n",
        "    syms.update(re.findall(r\"[A-Za-z]+_[A-Za-z0-9]+\", text))\n",
        "    syms.update(re.findall(r\"[A-Za-z]+\\^[0-9]+\", text))\n",
        "    # common math objects\n",
        "    syms.update(re.findall(r\"\\bGP\\b|\\bN\\b|\\bRMSE\\b|\\bAUC\\b|\\bROC\\b\", text))\n",
        "    # single-letter variables esp. with context (space or punctuation around)\n",
        "    syms.update(re.findall(r\"\\b[a-zA-Z]\\b\", text))\n",
        "    cleaned = sorted([s for s in syms if len(s)>0])\n",
        "    return cleaned\n",
        "\n",
        "symbols = set()\n",
        "for s in sections:\n",
        "    symbols.update(extract_symbols_from_text(s['text'] or ''))\n",
        "\n",
        "symbols = sorted(list(symbols))\n",
        "with open('symbol_list.json', 'w') as f:\n",
        "    json.dump(symbols, f, indent=2)\n",
        "print('Saved symbol_list.json')\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# 4) Heatmap: term occurrence by page/section\n",
        "# We'll build a matrix: rows=top-N terms, cols=page numbers, entries=count\n",
        "\n",
        "TOP_N = 30\n",
        "terms = global_freq_df['word'].head(TOP_N).tolist()\n",
        "num_pages = len(sections)\n",
        "mat = np.zeros((len(terms), num_pages), dtype=int)\n",
        "for i, t in enumerate(terms):\n",
        "    pat = re.compile(r\"\\\\b\" + re.escape(t) + r\"\\\\b\", flags=re.IGNORECASE)\n",
        "    for j, s in enumerate(sections):\n",
        "        mat[i, j] = len(pat.findall(s['text'] or ''))\n",
        "\n",
        "heat_df = pd.DataFrame(mat, index=terms, columns=[f'page_{p+1}' for p in range(num_pages)])\n",
        "heat_df.to_csv('term_page_heatmap.csv')\n",
        "\n",
        "# Plot / save heatmap using robust helper\n",
        "save_heatmap(mat, row_labels=terms, col_labels=[str(i+1) for i in range(num_pages)], filepath='term_heatmap.png')\n",
        "print('Saved term_page_heatmap.csv')\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Frequency bar chart for top 15\n",
        "TOP_SHOW = min(15, len(global_freq_df))\n",
        "top15 = global_freq_df.head(TOP_SHOW)\n",
        "labels = top15['word'].tolist()\n",
        "values = top15['frequency'].astype(int).tolist()\n",
        "save_bar_chart(labels, values, 'top15_frequency.png')\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Wordcloud (save to file via WordCloud.to_file)\n",
        "save_wordcloud_from_frequencies(global_freq_df.set_index('word')['frequency'], 'term_frequency_wordcloud.png')\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# 5) Reproducible pseudocode extraction for methods / algorithms\n",
        "ps_sections = []\n",
        "for s in sections:\n",
        "    text = s['text'] or ''\n",
        "    if re.search(r'algorithm|we propose|we define|procedure|update|posterior|prior|likelihood', text, flags=re.IGNORECASE):\n",
        "        ps_sections.append({'page': s['page'], 'text': text})\n",
        "\n",
        "with open('candidate_method_paragraphs.txt','w') as f:\n",
        "    for p in ps_sections:\n",
        "        f.write(f\"--- PAGE {p['page']} ---\\n\")\n",
        "        f.write(p['text'][:4000].replace('\\n','\\n') + '\\n\\n')\n",
        "\n",
        "print('Saved candidate_method_paragraphs.txt (for manual review)')\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# 6) Optional: query OpenAI for term definitions (user must review)\n",
        "# NOTE: This uses the OpenAI API. You must set OPENAI_API_KEY in the environment or pass it below.\n",
        "\n",
        "def query_openai_definitions(terms: List[str], api_key: str=None, model: str='gpt-4o-mini') -> Dict[str, str]:\n",
        "    if openai is None:\n",
        "        raise RuntimeError('openai package not installed')\n",
        "    if api_key is None:\n",
        "        api_key = os.environ.get('OPENAI_API_KEY')\n",
        "    if not api_key:\n",
        "        raise RuntimeError('OpenAI API key not found. Set OPENAI_API_KEY environment variable or pass api_key.')\n",
        "    openai.api_key = api_key\n",
        "    results = {}\n",
        "    for term in terms:\n",
        "        prompt = f\"Provide a concise (<= 60 words) technical definition of the term '{term}' as used in machine learning / statistical modeling. Include one-sentence note about typical pitfalls or assumptions. Respond in JSON with keys: term, definition, caveat.\"\n",
        "        try:\n",
        "            # Use ChatCompletion if available; this call may need adjustment depending on openai package version\n",
        "            resp = openai.ChatCompletion.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "                max_tokens=200,\n",
        "                temperature=0.0,\n",
        "            )\n",
        "            out = resp['choices'][0]['message']['content']\n",
        "            results[term] = out\n",
        "        except Exception as e:\n",
        "            results[term] = f'ERROR: {e}'\n",
        "    return results\n",
        "\n",
        "# Example usage (commented):\n",
        "# defs = query_openai_definitions([t['term'] for t in term_dict[:20]], api_key='YOUR_API_KEY_HERE')\n",
        "# with open('llm_term_definitions.json','w') as f:\n",
        "#     json.dump(defs, f, indent=2)\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# 7) Small utility: generate a markdown report that summarizes findings and links files\n",
        "report_md = []\n",
        "report_md.append('# Auto-generated parsing report')\n",
        "report_md.append('\\n')\n",
        "report_md.append('Files saved in workspace:')\n",
        "report_md.append('- global_frequency.csv')\n",
        "report_md.append('- term_summary.csv')\n",
        "report_md.append('- term_dictionary.json')\n",
        "report_md.append('- symbol_list.json')\n",
        "report_md.append('- term_page_heatmap.csv')\n",
        "report_md.append('- term_heatmap.png (or fallback CSV/HTML)')\n",
        "report_md.append('- top15_frequency.png (or fallback CSV/HTML)')\n",
        "report_md.append('- term_frequency_wordcloud.png (or fallback CSV)')\n",
        "\n",
        "with open('parsing_report.md','w') as f:\n",
        "    f.write('\\n'.join(report_md))\n",
        "print('Saved parsing_report.md')\n",
        "\n",
        "print('All done. Inspect CSV/JSON outputs and review candidate method paragraphs before using LLM to generate definitions.')\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# NEXT STEPS (manual review + interactive workflow):\n",
        "# - Open term_summary.csv and term_dictionary.json to review context snippets\n",
        "# - Edit/curate the list of terms you want definitions for\n",
        "# - Use query_openai_definitions(...) to request definitions from the ChatGPT API\n",
        "# - Save the LLM outputs and manually accept/reject/modify definitions before using them in your notes\n",
        "\n",
        "# End of Colab-ready script (debugged to handle missing matplotlib)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eG3-MOZv_E10"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}